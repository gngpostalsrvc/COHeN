{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"0c5eaad8b77e4b59b396c0cd847551c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f14d63878c61475eb41a69097283a97c","IPY_MODEL_6fcf52e4db174987996c52cd43114151","IPY_MODEL_0c6d086a5853474a8ce58344b303ca46"],"layout":"IPY_MODEL_a47d5ac8d6684adf93afcbd24efb2d19"}},"f14d63878c61475eb41a69097283a97c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee4b7a857b554269b2380a76b80fc83f","placeholder":"​","style":"IPY_MODEL_8e9083811e9c4f00b1225971f8c87a07","value":"Downloading (…)okenizer_config.json: 100%"}},"6fcf52e4db174987996c52cd43114151":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56ee130b7b2942669d52259aa41b724f","max":51,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8da23e5faf0b4ee08bbd0f7074b65e6b","value":51}},"0c6d086a5853474a8ce58344b303ca46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b24ecce5b6b54333bd669c2a7b9c88c3","placeholder":"​","style":"IPY_MODEL_0d4b0e880dac424ab5bf1138b44c7002","value":" 51.0/51.0 [00:00&lt;00:00, 1.30kB/s]"}},"a47d5ac8d6684adf93afcbd24efb2d19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ee4b7a857b554269b2380a76b80fc83f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e9083811e9c4f00b1225971f8c87a07":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56ee130b7b2942669d52259aa41b724f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8da23e5faf0b4ee08bbd0f7074b65e6b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b24ecce5b6b54333bd669c2a7b9c88c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d4b0e880dac424ab5bf1138b44c7002":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"001d51c7bacf4fdbb73efffd64cf8802":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_887d814f745a4b6b938e23a335acd40c","IPY_MODEL_cda29b737de84ecbb53aea408b4a6ab6","IPY_MODEL_102dcc3c2bb546359c9801f6b56152a6"],"layout":"IPY_MODEL_fb641843873647e889cd1aa8698bab7b"}},"887d814f745a4b6b938e23a335acd40c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3607f882c34aa98a10a2aba7321c71","placeholder":"​","style":"IPY_MODEL_2f6c58a87d6d4393a9d967d1fc2a7b63","value":"Downloading (…)/main/tokenizer.json: 100%"}},"cda29b737de84ecbb53aea408b4a6ab6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fcee3bca05ac4358b048fac5feee7b12","max":109646,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9d38aaa7b85b4cbc9d9b800bff0062cf","value":109646}},"102dcc3c2bb546359c9801f6b56152a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65be6587d825483ab8195d7bd9b2d545","placeholder":"​","style":"IPY_MODEL_ed59b2760f154549ade8a558d0f51691","value":" 110k/110k [00:00&lt;00:00, 530kB/s]"}},"fb641843873647e889cd1aa8698bab7b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc3607f882c34aa98a10a2aba7321c71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f6c58a87d6d4393a9d967d1fc2a7b63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fcee3bca05ac4358b048fac5feee7b12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d38aaa7b85b4cbc9d9b800bff0062cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65be6587d825483ab8195d7bd9b2d545":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed59b2760f154549ade8a558d0f51691":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2ff1fa9700a443bae6b72cfee3caaa2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e9e18fc156b14e45825deab103ca8994","IPY_MODEL_63146ee4a0c24c73b4c96ae00c7243cf","IPY_MODEL_48671b7ad1524ca7ad3b4bd8b15c5db9"],"layout":"IPY_MODEL_6119b131753d487881a0912525d589f7"}},"e9e18fc156b14e45825deab103ca8994":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_339f7dbfbea841c8848e23b9b4b781cd","placeholder":"​","style":"IPY_MODEL_aaa95fd7d119414b81a353cb0f2f02d1","value":"Downloading (…)cial_tokens_map.json: 100%"}},"63146ee4a0c24c73b4c96ae00c7243cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c5f5b5c47324cc382798f2dccab6894","max":121,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8170778ed07b480a8217d7dad82cb6ad","value":121}},"48671b7ad1524ca7ad3b4bd8b15c5db9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48091efc4f6b4c8a9e6f8fd855f9262e","placeholder":"​","style":"IPY_MODEL_d2fa349b78b74b2cb40dfa3e19ecd56c","value":" 121/121 [00:00&lt;00:00, 4.42kB/s]"}},"6119b131753d487881a0912525d589f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"339f7dbfbea841c8848e23b9b4b781cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa95fd7d119414b81a353cb0f2f02d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c5f5b5c47324cc382798f2dccab6894":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8170778ed07b480a8217d7dad82cb6ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"48091efc4f6b4c8a9e6f8fd855f9262e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2fa349b78b74b2cb40dfa3e19ecd56c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9029fd7619634a6cb8836132e48954cc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f864fd0a659464a8f34f6900fda9b81","IPY_MODEL_3cc6c2299ffc4676a2e27081eca2ff29","IPY_MODEL_7b764778281347e091c559f778f2e04e"],"layout":"IPY_MODEL_3b9ad67d28724d059b895a620b27cf24"}},"8f864fd0a659464a8f34f6900fda9b81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd67826d8d9b4db38a2d02a6932b4994","placeholder":"​","style":"IPY_MODEL_59e1e586ad2541f8a57ea2392a93bdbe","value":"Downloading metadata: 100%"}},"3cc6c2299ffc4676a2e27081eca2ff29":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb7cbee4e089417fb20b9e3c5af08dc5","max":916,"min":0,"orientation":"horizontal","style":"IPY_MODEL_523331a03f714a16be75a880c2cb1a1b","value":916}},"7b764778281347e091c559f778f2e04e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea6fd390579346e0a9fd95e7a8fec670","placeholder":"​","style":"IPY_MODEL_ce3824a7128147b7b8f58f39c5c3d173","value":" 916/916 [00:00&lt;00:00, 22.3kB/s]"}},"3b9ad67d28724d059b895a620b27cf24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd67826d8d9b4db38a2d02a6932b4994":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59e1e586ad2541f8a57ea2392a93bdbe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb7cbee4e089417fb20b9e3c5af08dc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"523331a03f714a16be75a880c2cb1a1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ea6fd390579346e0a9fd95e7a8fec670":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce3824a7128147b7b8f58f39c5c3d173":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb6cc03159e34364b6f5e5e92f5b924c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3f467db6557b48e6a992811d2d201f82","IPY_MODEL_c2f4ffcfa5cc4f72a03708c9bc1a7f8a","IPY_MODEL_a464c5c25d304443a603cbc378fd541b"],"layout":"IPY_MODEL_88a1f046fd3b4c57818b2e2691c99feb"}},"3f467db6557b48e6a992811d2d201f82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c76d2327068745b9bae571b9088f9619","placeholder":"​","style":"IPY_MODEL_a619e94faf5347218a50e6e9957d45db","value":"Downloading data files: 100%"}},"c2f4ffcfa5cc4f72a03708c9bc1a7f8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e7b031bab69449b98cd43e4e850a533a","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b822b8634fa4614b8678038f8ecf49b","value":3}},"a464c5c25d304443a603cbc378fd541b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba53806c0e8b4071a99a97a1e90e3730","placeholder":"​","style":"IPY_MODEL_f631ea7c6e0b4c5590f5c491b3483758","value":" 3/3 [00:05&lt;00:00,  1.98s/it]"}},"88a1f046fd3b4c57818b2e2691c99feb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c76d2327068745b9bae571b9088f9619":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a619e94faf5347218a50e6e9957d45db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7b031bab69449b98cd43e4e850a533a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b822b8634fa4614b8678038f8ecf49b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba53806c0e8b4071a99a97a1e90e3730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f631ea7c6e0b4c5590f5c491b3483758":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b771a6c2146143c7a90a8f9a9e675960":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_831f57fe46b54e8eac6b16d12ec02fbe","IPY_MODEL_5d082923c57745cea4cd11994ae34c5b","IPY_MODEL_355bc584eda54672a5edf625262f85e9"],"layout":"IPY_MODEL_cf60cc21914742a1b7afd39103a5a9e2"}},"831f57fe46b54e8eac6b16d12ec02fbe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c0ea268a2eb4de496657f4c54b62cc7","placeholder":"​","style":"IPY_MODEL_111662bd40a94b4aadb2e20ebb7e5cf8","value":"Downloading data: 100%"}},"5d082923c57745cea4cd11994ae34c5b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bf8eb39430af43b1858b15f3a5abeff6","max":119634,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f8869cbd73774eb6b1cc49c831a39e40","value":119634}},"355bc584eda54672a5edf625262f85e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2c7ef0606aa41c9b50ea0319bce5023","placeholder":"​","style":"IPY_MODEL_aaa2ed2d66a54dd1bb81caef8cc2a361","value":" 120k/120k [00:00&lt;00:00, 310kB/s]"}},"cf60cc21914742a1b7afd39103a5a9e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c0ea268a2eb4de496657f4c54b62cc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"111662bd40a94b4aadb2e20ebb7e5cf8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf8eb39430af43b1858b15f3a5abeff6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8869cbd73774eb6b1cc49c831a39e40":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2c7ef0606aa41c9b50ea0319bce5023":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aaa2ed2d66a54dd1bb81caef8cc2a361":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91d4230c14754054ab74971daa0497d5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_92e9acac441c432a8fb281ab00974606","IPY_MODEL_3e973f11806e4c16a50c1fe52b59b0e4","IPY_MODEL_4a50f6d21d9846fd95f10c8947166013"],"layout":"IPY_MODEL_33a0828ca4774e8d9116ef2294d8bc90"}},"92e9acac441c432a8fb281ab00974606":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5308f715f8204f6585a8816749b85ac3","placeholder":"​","style":"IPY_MODEL_8a87390c2823457786375847ea41675e","value":"Downloading data: 100%"}},"3e973f11806e4c16a50c1fe52b59b0e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4493d9c5eb4e4157b91de79e2cca62b7","max":121270,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a75cf4e3ed44404797a7c13700e240db","value":121270}},"4a50f6d21d9846fd95f10c8947166013":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2fa8abb4ae3d45928840d428972c5ada","placeholder":"​","style":"IPY_MODEL_eb66201cd00b455a9c4918d73c03dc04","value":" 121k/121k [00:00&lt;00:00, 301kB/s]"}},"33a0828ca4774e8d9116ef2294d8bc90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5308f715f8204f6585a8816749b85ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a87390c2823457786375847ea41675e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4493d9c5eb4e4157b91de79e2cca62b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75cf4e3ed44404797a7c13700e240db":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2fa8abb4ae3d45928840d428972c5ada":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb66201cd00b455a9c4918d73c03dc04":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a703c2b465ba40afb92959ac3ecaf194":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ba567ac1b784ba1834d5b520b4aa4cb","IPY_MODEL_e11f75eac2aa45b494b7b175686d1535","IPY_MODEL_534e8db45af04ee99822328ce77fa486"],"layout":"IPY_MODEL_11d536fcaa4f4291bdd6f293a8c22f81"}},"6ba567ac1b784ba1834d5b520b4aa4cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9a44b801e22c45fba58c226439cbc18a","placeholder":"​","style":"IPY_MODEL_8b1680d75a1c4afbb34dca118c59fb63","value":"Downloading data: 100%"}},"e11f75eac2aa45b494b7b175686d1535":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_845431fb25504d738999aaddd6b933a8","max":890218,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a7cd38184c2e4ee99d06a60ae08198f6","value":890218}},"534e8db45af04ee99822328ce77fa486":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8248efd645a476787a6570d9f35d099","placeholder":"​","style":"IPY_MODEL_53a87204734f473ba98094b21b65e3cc","value":" 890k/890k [00:00&lt;00:00, 1.74MB/s]"}},"11d536fcaa4f4291bdd6f293a8c22f81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a44b801e22c45fba58c226439cbc18a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b1680d75a1c4afbb34dca118c59fb63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"845431fb25504d738999aaddd6b933a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7cd38184c2e4ee99d06a60ae08198f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8248efd645a476787a6570d9f35d099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53a87204734f473ba98094b21b65e3cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ef90980a9cf404eaa3f27798fa29d60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45e9af4feca24186afeff4721e78641d","IPY_MODEL_4996907536a24ad0ac177919acb0a8da","IPY_MODEL_3f17b2066cdb41a3a725018bcb668faf"],"layout":"IPY_MODEL_d59a139e43034cd0904ea916fae86072"}},"45e9af4feca24186afeff4721e78641d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd9a0567d2864564a57a676e52f31a4c","placeholder":"​","style":"IPY_MODEL_2d7f4d1f25ba45ea9e857d580c75c66f","value":"Extracting data files: 100%"}},"4996907536a24ad0ac177919acb0a8da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fe8efd2748e411e8fe70480cb9fc2c0","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc82155bcc374f548b1ea6ccfa044848","value":3}},"3f17b2066cdb41a3a725018bcb668faf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_541d83618db94b17bfde671ae0f99b85","placeholder":"​","style":"IPY_MODEL_f48be8a904784b219c6061dd8817868a","value":" 3/3 [00:00&lt;00:00, 63.85it/s]"}},"d59a139e43034cd0904ea916fae86072":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9a0567d2864564a57a676e52f31a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d7f4d1f25ba45ea9e857d580c75c66f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fe8efd2748e411e8fe70480cb9fc2c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc82155bcc374f548b1ea6ccfa044848":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"541d83618db94b17bfde671ae0f99b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f48be8a904784b219c6061dd8817868a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6eb91d6b584e48878d40aac73b2eddb1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9af6d32390494157a476f223c01edef7","IPY_MODEL_bbd76d75172345f584d2687c8dec6759","IPY_MODEL_1212714017164f359f01514cd921a852"],"layout":"IPY_MODEL_d2e5b15a9865471a982d22e8e9d9716a"}},"9af6d32390494157a476f223c01edef7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_365024be3d20481e9d0e34f375e2c15c","placeholder":"​","style":"IPY_MODEL_d80d552896e24f9ea2eeb22cbdec9309","value":"Generating test split:   0%"}},"bbd76d75172345f584d2687c8dec6759":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_7714c64995b54c9fa8bb11d977448497","max":1197,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a5621044fece40cb8b256fd4d5eee109","value":1197}},"1212714017164f359f01514cd921a852":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5626b55a77d24bf3bf2399467a5c75e4","placeholder":"​","style":"IPY_MODEL_208929d212a94f0292193e8b6d1ec1ea","value":" 0/1197 [00:00&lt;?, ? examples/s]"}},"d2e5b15a9865471a982d22e8e9d9716a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"365024be3d20481e9d0e34f375e2c15c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d80d552896e24f9ea2eeb22cbdec9309":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7714c64995b54c9fa8bb11d977448497":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5621044fece40cb8b256fd4d5eee109":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5626b55a77d24bf3bf2399467a5c75e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"208929d212a94f0292193e8b6d1ec1ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21edef14c2cc41c580eec1f3d5361277":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8e9848da7bb47e5a26dfcbc63765dc8","IPY_MODEL_5246d52a89b74832822e39a6ff7f4c34","IPY_MODEL_e6b4a4ad6eed46bf9e1cf8545ae5e1a7"],"layout":"IPY_MODEL_7f6a36d85bd04164b8cfdf5f48839cc1"}},"a8e9848da7bb47e5a26dfcbc63765dc8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b92d607fad7452b8f1e729717cfbe76","placeholder":"​","style":"IPY_MODEL_6a59937f1e1449afae200b9bae631f2f","value":"Generating eval split:   0%"}},"5246d52a89b74832822e39a6ff7f4c34":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_a72edbd86ffb444abccfdde6f91ed32d","max":1197,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7425506e20f84925b34bf950bddbb8ce","value":1197}},"e6b4a4ad6eed46bf9e1cf8545ae5e1a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0cf8137e216488ca45d94dc573f64f1","placeholder":"​","style":"IPY_MODEL_589112571f3c45078e62d56e3540d875","value":" 0/1197 [00:00&lt;?, ? examples/s]"}},"7f6a36d85bd04164b8cfdf5f48839cc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"4b92d607fad7452b8f1e729717cfbe76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a59937f1e1449afae200b9bae631f2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a72edbd86ffb444abccfdde6f91ed32d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7425506e20f84925b34bf950bddbb8ce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0cf8137e216488ca45d94dc573f64f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"589112571f3c45078e62d56e3540d875":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97ccd5ed390744bba6a55be39ac27deb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1b9bf1f92ba407fadbc5fa793567e2e","IPY_MODEL_80b01ca8aa874f389b09e6ed3de9385d","IPY_MODEL_5b2638c4af274a5f8d419da1fdb8b054"],"layout":"IPY_MODEL_f35ff12e56274fc3bbd01bfe1edfb5bd"}},"c1b9bf1f92ba407fadbc5fa793567e2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fb896d0a1794b14a596cee749c12fdc","placeholder":"​","style":"IPY_MODEL_34f5ef93372d4db59f3b481fb09d5715","value":"Generating train split: 100%"}},"80b01ca8aa874f389b09e6ed3de9385d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc0d3c691ba14a0d82a89d3aee1d210b","max":9574,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7312c7a00d484da686739a4ec08b646e","value":9574}},"5b2638c4af274a5f8d419da1fdb8b054":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d591b1f58b84e448dbf44c3672abd80","placeholder":"​","style":"IPY_MODEL_a0e0421004fd4d14818eb91005e3e9ab","value":" 9574/9574 [00:00&lt;00:00, 82157.80 examples/s]"}},"f35ff12e56274fc3bbd01bfe1edfb5bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9fb896d0a1794b14a596cee749c12fdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34f5ef93372d4db59f3b481fb09d5715":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc0d3c691ba14a0d82a89d3aee1d210b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7312c7a00d484da686739a4ec08b646e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d591b1f58b84e448dbf44c3672abd80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0e0421004fd4d14818eb91005e3e9ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6980469b22854ee8956e288cb5f49caa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5fbb9452c51943cb8beeb8053d390c67","IPY_MODEL_01ae799270154799ac3808aa0e71300d","IPY_MODEL_3556d3bdfee9425aa366fe1636128534"],"layout":"IPY_MODEL_1fdf518b9bab4f92b288b2693d51b253"}},"5fbb9452c51943cb8beeb8053d390c67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_234c8a19672e412b80945caa16bcb44f","placeholder":"​","style":"IPY_MODEL_15441c9aa7ec47209667bb2745dd7719","value":"100%"}},"01ae799270154799ac3808aa0e71300d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbaa4dedff904729b6b0be668fe24414","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_adf4fc9c57394fb4b5f4d7a04f40ccbc","value":3}},"3556d3bdfee9425aa366fe1636128534":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa3589fd90af462a8d1bc684297e9991","placeholder":"​","style":"IPY_MODEL_ff88219bdaa347be94c72558adc48cc1","value":" 3/3 [00:00&lt;00:00, 36.88it/s]"}},"1fdf518b9bab4f92b288b2693d51b253":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"234c8a19672e412b80945caa16bcb44f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15441c9aa7ec47209667bb2745dd7719":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cbaa4dedff904729b6b0be668fe24414":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adf4fc9c57394fb4b5f4d7a04f40ccbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aa3589fd90af462a8d1bc684297e9991":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff88219bdaa347be94c72558adc48cc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"553147f17d1f484881ae2687349045f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2a96ee5e1e84ca49b7673fa97c470cc","IPY_MODEL_bb74eef647f74043a6806ee54ddc7339","IPY_MODEL_605d502b01e043f588737cb0d698ca79"],"layout":"IPY_MODEL_bf9b5903b24441b7bf8f0454320adfd1"}},"c2a96ee5e1e84ca49b7673fa97c470cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d2e54d9be2a4ce496cd7de73cf38ed5","placeholder":"​","style":"IPY_MODEL_6171526bfb0f4170b1ee7f44f31655ee","value":"Map:  84%"}},"bb74eef647f74043a6806ee54ddc7339":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e2db0ab244f42c4803e3f01b77ed983","max":1197,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0a3c9378591649cf8d8d004b56a80eea","value":1197}},"605d502b01e043f588737cb0d698ca79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0db609f86ee041caab350fefe4aceebf","placeholder":"​","style":"IPY_MODEL_d07ba7b603b64e639e0acdf974e25a69","value":" 1000/1197 [00:00&lt;00:00, 2433.78 examples/s]"}},"bf9b5903b24441b7bf8f0454320adfd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"9d2e54d9be2a4ce496cd7de73cf38ed5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6171526bfb0f4170b1ee7f44f31655ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e2db0ab244f42c4803e3f01b77ed983":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a3c9378591649cf8d8d004b56a80eea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0db609f86ee041caab350fefe4aceebf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d07ba7b603b64e639e0acdf974e25a69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ffe094769cc941818ece5814b91d5d23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26e833371c8e477a9e47164126fd1188","IPY_MODEL_6614eee6d2bc4d9ebb3a39e70459646e","IPY_MODEL_91f1889c911e47acbe2f8572c0e35f7b"],"layout":"IPY_MODEL_410a6049403449e0a4b1250ea6574e44"}},"26e833371c8e477a9e47164126fd1188":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c1e86fc2e39249db815537544269a937","placeholder":"​","style":"IPY_MODEL_a564e3c1910d42e3bd684c66f3cc32ba","value":"Map:  84%"}},"6614eee6d2bc4d9ebb3a39e70459646e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_59c94ccc796c431ab268f3ef85703cff","max":1197,"min":0,"orientation":"horizontal","style":"IPY_MODEL_45e3a13f81974cf296c5f948dd9a4e95","value":1197}},"91f1889c911e47acbe2f8572c0e35f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63bf53367320453c89bbf997a73d746a","placeholder":"​","style":"IPY_MODEL_4f786246fada4ae8b2dcc89913499ff8","value":" 1000/1197 [00:00&lt;00:00, 2232.74 examples/s]"}},"410a6049403449e0a4b1250ea6574e44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"c1e86fc2e39249db815537544269a937":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a564e3c1910d42e3bd684c66f3cc32ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"59c94ccc796c431ab268f3ef85703cff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45e3a13f81974cf296c5f948dd9a4e95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"63bf53367320453c89bbf997a73d746a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f786246fada4ae8b2dcc89913499ff8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac8d0b2a93f342ce9e667fdae283f244":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6a72aa96082a4f16aa7c36fe0a943df6","IPY_MODEL_163e6060218941d98787102261af36c2","IPY_MODEL_822fafba68b449a5bfa043643880e818"],"layout":"IPY_MODEL_75aa5b5c4d9d4480b2454f027d697152"}},"6a72aa96082a4f16aa7c36fe0a943df6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49d88c66b5324e9eb330678e8d54f707","placeholder":"​","style":"IPY_MODEL_cf48e40c9db7419586181a3c53f88617","value":"Map: 100%"}},"163e6060218941d98787102261af36c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cb7d5505ed643edb80d4df216e060d3","max":9574,"min":0,"orientation":"horizontal","style":"IPY_MODEL_04f3bd47cd814424a5f56bb3ed5098a7","value":9574}},"822fafba68b449a5bfa043643880e818":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_943456a931484e0d978e49be6f797624","placeholder":"​","style":"IPY_MODEL_2b44eb69c3804c0aa4839902be9c716f","value":" 9574/9574 [00:03&lt;00:00, 2968.98 examples/s]"}},"75aa5b5c4d9d4480b2454f027d697152":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":null}},"49d88c66b5324e9eb330678e8d54f707":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf48e40c9db7419586181a3c53f88617":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4cb7d5505ed643edb80d4df216e060d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04f3bd47cd814424a5f56bb3ed5098a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"943456a931484e0d978e49be6f797624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b44eb69c3804c0aa4839902be9c716f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d03b87b5e0f9496b90a813e1940639dd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6574990ea8c24f1285dcda77095ebb7f","IPY_MODEL_d31909a4e8ee4ffd8fcba374138b533b","IPY_MODEL_c5fc3829dfff450c9c2edf76330be8d0"],"layout":"IPY_MODEL_c69c862256604dc981304d4b89d91d7d"}},"6574990ea8c24f1285dcda77095ebb7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3950c92254094a38a9fc865f1b118d19","placeholder":"​","style":"IPY_MODEL_042bc4e51e954fdc8d715179c1939d52","value":"Downloading (…)lve/main/config.json: 100%"}},"d31909a4e8ee4ffd8fcba374138b533b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2567dfbd802c4f2db8d643b562591a59","max":669,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eab7956a6f7444a79fa266b073772d44","value":669}},"c5fc3829dfff450c9c2edf76330be8d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_060ec577a1a446fda25244cdb8a81bcc","placeholder":"​","style":"IPY_MODEL_fbeb953e994e4aaa9fb57662c115856c","value":" 669/669 [00:00&lt;00:00, 12.5kB/s]"}},"c69c862256604dc981304d4b89d91d7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3950c92254094a38a9fc865f1b118d19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"042bc4e51e954fdc8d715179c1939d52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2567dfbd802c4f2db8d643b562591a59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eab7956a6f7444a79fa266b073772d44":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"060ec577a1a446fda25244cdb8a81bcc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbeb953e994e4aaa9fb57662c115856c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"165eea738200474aaf76e3b7378b21a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf052865ef56467fbcdcfef264b430d8","IPY_MODEL_bd73076bbbf446bfb62c21ffa2efcb32","IPY_MODEL_c57635badef046c594539f1f7b864f20"],"layout":"IPY_MODEL_56d22c04089f474ba4efac0e9348c83a"}},"cf052865ef56467fbcdcfef264b430d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a788a7ba655d47dba1a5156d0afb8ec0","placeholder":"​","style":"IPY_MODEL_002f0d340c0546acaae8c91a6cdfee2f","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"bd73076bbbf446bfb62c21ffa2efcb32":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2671b7984854ca0ae2656ce62ee59a1","max":5675650,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e8a7bf250f2e4dd78207d5bede4fce91","value":5675650}},"c57635badef046c594539f1f7b864f20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10e2252355944b4b85215680a10f4030","placeholder":"​","style":"IPY_MODEL_1decf84b8f294c419f7f318a127a17d8","value":" 5.68M/5.68M [00:00&lt;00:00, 7.79MB/s]"}},"56d22c04089f474ba4efac0e9348c83a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a788a7ba655d47dba1a5156d0afb8ec0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"002f0d340c0546acaae8c91a6cdfee2f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2671b7984854ca0ae2656ce62ee59a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8a7bf250f2e4dd78207d5bede4fce91":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"10e2252355944b4b85215680a10f4030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1decf84b8f294c419f7f318a127a17d8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qiSzMeSfPkQU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677012546736,"user_tz":360,"elapsed":1021,"user":{"displayName":"Aren Wilson-Wright","userId":"05722690186564388330"}},"outputId":"7159cede-a165-4f34-ebda-c2b8b03865f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU 0: Tesla T4 (UUID: GPU-52361079-3dd1-307a-4402-e81341128dc7)\n"]}],"source":["!nvidia-smi -L"]},{"cell_type":"code","source":["%%capture install_log \n","\n","!pip install transformers datasets evaluate optuna"],"metadata":{"id":"WyzxqKhpPpKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import optuna\n","import evaluate\n","import numpy as np\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    DataCollatorForLanguageModeling,\n","    PreTrainedTokenizerFast,\n","    Trainer,\n","    TrainingArguments\n",")\n","from datasets import load_dataset"],"metadata":{"id":"M9zQojvuPrex"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = PreTrainedTokenizerFast.from_pretrained('gngpostalsrvc/BERiT_2000_custom_architecture_150_epochs_2')\n","\n","def preprocess(examples):\n","  \n","    encoding = tokenizer(examples['Text'], max_length=128, truncation=True, padding=True)\n","    encoding['labels'] = [[stage] for stage in examples['Stage']]\n","\n","    return encoding\n","\n","raw_data = load_dataset('gngpostalsrvc/COHeN')\n","\n","tokenized_data = raw_data.map(preprocess, batched=True, remove_columns=raw_data['train'].column_names)\n","tokenized_data.set_format(\"pt\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], output_all_columns=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394,"referenced_widgets":["0c5eaad8b77e4b59b396c0cd847551c7","f14d63878c61475eb41a69097283a97c","6fcf52e4db174987996c52cd43114151","0c6d086a5853474a8ce58344b303ca46","a47d5ac8d6684adf93afcbd24efb2d19","ee4b7a857b554269b2380a76b80fc83f","8e9083811e9c4f00b1225971f8c87a07","56ee130b7b2942669d52259aa41b724f","8da23e5faf0b4ee08bbd0f7074b65e6b","b24ecce5b6b54333bd669c2a7b9c88c3","0d4b0e880dac424ab5bf1138b44c7002","001d51c7bacf4fdbb73efffd64cf8802","887d814f745a4b6b938e23a335acd40c","cda29b737de84ecbb53aea408b4a6ab6","102dcc3c2bb546359c9801f6b56152a6","fb641843873647e889cd1aa8698bab7b","cc3607f882c34aa98a10a2aba7321c71","2f6c58a87d6d4393a9d967d1fc2a7b63","fcee3bca05ac4358b048fac5feee7b12","9d38aaa7b85b4cbc9d9b800bff0062cf","65be6587d825483ab8195d7bd9b2d545","ed59b2760f154549ade8a558d0f51691","d2ff1fa9700a443bae6b72cfee3caaa2","e9e18fc156b14e45825deab103ca8994","63146ee4a0c24c73b4c96ae00c7243cf","48671b7ad1524ca7ad3b4bd8b15c5db9","6119b131753d487881a0912525d589f7","339f7dbfbea841c8848e23b9b4b781cd","aaa95fd7d119414b81a353cb0f2f02d1","3c5f5b5c47324cc382798f2dccab6894","8170778ed07b480a8217d7dad82cb6ad","48091efc4f6b4c8a9e6f8fd855f9262e","d2fa349b78b74b2cb40dfa3e19ecd56c","9029fd7619634a6cb8836132e48954cc","8f864fd0a659464a8f34f6900fda9b81","3cc6c2299ffc4676a2e27081eca2ff29","7b764778281347e091c559f778f2e04e","3b9ad67d28724d059b895a620b27cf24","cd67826d8d9b4db38a2d02a6932b4994","59e1e586ad2541f8a57ea2392a93bdbe","bb7cbee4e089417fb20b9e3c5af08dc5","523331a03f714a16be75a880c2cb1a1b","ea6fd390579346e0a9fd95e7a8fec670","ce3824a7128147b7b8f58f39c5c3d173","bb6cc03159e34364b6f5e5e92f5b924c","3f467db6557b48e6a992811d2d201f82","c2f4ffcfa5cc4f72a03708c9bc1a7f8a","a464c5c25d304443a603cbc378fd541b","88a1f046fd3b4c57818b2e2691c99feb","c76d2327068745b9bae571b9088f9619","a619e94faf5347218a50e6e9957d45db","e7b031bab69449b98cd43e4e850a533a","3b822b8634fa4614b8678038f8ecf49b","ba53806c0e8b4071a99a97a1e90e3730","f631ea7c6e0b4c5590f5c491b3483758","b771a6c2146143c7a90a8f9a9e675960","831f57fe46b54e8eac6b16d12ec02fbe","5d082923c57745cea4cd11994ae34c5b","355bc584eda54672a5edf625262f85e9","cf60cc21914742a1b7afd39103a5a9e2","4c0ea268a2eb4de496657f4c54b62cc7","111662bd40a94b4aadb2e20ebb7e5cf8","bf8eb39430af43b1858b15f3a5abeff6","f8869cbd73774eb6b1cc49c831a39e40","e2c7ef0606aa41c9b50ea0319bce5023","aaa2ed2d66a54dd1bb81caef8cc2a361","91d4230c14754054ab74971daa0497d5","92e9acac441c432a8fb281ab00974606","3e973f11806e4c16a50c1fe52b59b0e4","4a50f6d21d9846fd95f10c8947166013","33a0828ca4774e8d9116ef2294d8bc90","5308f715f8204f6585a8816749b85ac3","8a87390c2823457786375847ea41675e","4493d9c5eb4e4157b91de79e2cca62b7","a75cf4e3ed44404797a7c13700e240db","2fa8abb4ae3d45928840d428972c5ada","eb66201cd00b455a9c4918d73c03dc04","a703c2b465ba40afb92959ac3ecaf194","6ba567ac1b784ba1834d5b520b4aa4cb","e11f75eac2aa45b494b7b175686d1535","534e8db45af04ee99822328ce77fa486","11d536fcaa4f4291bdd6f293a8c22f81","9a44b801e22c45fba58c226439cbc18a","8b1680d75a1c4afbb34dca118c59fb63","845431fb25504d738999aaddd6b933a8","a7cd38184c2e4ee99d06a60ae08198f6","c8248efd645a476787a6570d9f35d099","53a87204734f473ba98094b21b65e3cc","7ef90980a9cf404eaa3f27798fa29d60","45e9af4feca24186afeff4721e78641d","4996907536a24ad0ac177919acb0a8da","3f17b2066cdb41a3a725018bcb668faf","d59a139e43034cd0904ea916fae86072","cd9a0567d2864564a57a676e52f31a4c","2d7f4d1f25ba45ea9e857d580c75c66f","7fe8efd2748e411e8fe70480cb9fc2c0","bc82155bcc374f548b1ea6ccfa044848","541d83618db94b17bfde671ae0f99b85","f48be8a904784b219c6061dd8817868a","6eb91d6b584e48878d40aac73b2eddb1","9af6d32390494157a476f223c01edef7","bbd76d75172345f584d2687c8dec6759","1212714017164f359f01514cd921a852","d2e5b15a9865471a982d22e8e9d9716a","365024be3d20481e9d0e34f375e2c15c","d80d552896e24f9ea2eeb22cbdec9309","7714c64995b54c9fa8bb11d977448497","a5621044fece40cb8b256fd4d5eee109","5626b55a77d24bf3bf2399467a5c75e4","208929d212a94f0292193e8b6d1ec1ea","21edef14c2cc41c580eec1f3d5361277","a8e9848da7bb47e5a26dfcbc63765dc8","5246d52a89b74832822e39a6ff7f4c34","e6b4a4ad6eed46bf9e1cf8545ae5e1a7","7f6a36d85bd04164b8cfdf5f48839cc1","4b92d607fad7452b8f1e729717cfbe76","6a59937f1e1449afae200b9bae631f2f","a72edbd86ffb444abccfdde6f91ed32d","7425506e20f84925b34bf950bddbb8ce","a0cf8137e216488ca45d94dc573f64f1","589112571f3c45078e62d56e3540d875","97ccd5ed390744bba6a55be39ac27deb","c1b9bf1f92ba407fadbc5fa793567e2e","80b01ca8aa874f389b09e6ed3de9385d","5b2638c4af274a5f8d419da1fdb8b054","f35ff12e56274fc3bbd01bfe1edfb5bd","9fb896d0a1794b14a596cee749c12fdc","34f5ef93372d4db59f3b481fb09d5715","bc0d3c691ba14a0d82a89d3aee1d210b","7312c7a00d484da686739a4ec08b646e","8d591b1f58b84e448dbf44c3672abd80","a0e0421004fd4d14818eb91005e3e9ab","6980469b22854ee8956e288cb5f49caa","5fbb9452c51943cb8beeb8053d390c67","01ae799270154799ac3808aa0e71300d","3556d3bdfee9425aa366fe1636128534","1fdf518b9bab4f92b288b2693d51b253","234c8a19672e412b80945caa16bcb44f","15441c9aa7ec47209667bb2745dd7719","cbaa4dedff904729b6b0be668fe24414","adf4fc9c57394fb4b5f4d7a04f40ccbc","aa3589fd90af462a8d1bc684297e9991","ff88219bdaa347be94c72558adc48cc1","553147f17d1f484881ae2687349045f1","c2a96ee5e1e84ca49b7673fa97c470cc","bb74eef647f74043a6806ee54ddc7339","605d502b01e043f588737cb0d698ca79","bf9b5903b24441b7bf8f0454320adfd1","9d2e54d9be2a4ce496cd7de73cf38ed5","6171526bfb0f4170b1ee7f44f31655ee","0e2db0ab244f42c4803e3f01b77ed983","0a3c9378591649cf8d8d004b56a80eea","0db609f86ee041caab350fefe4aceebf","d07ba7b603b64e639e0acdf974e25a69","ffe094769cc941818ece5814b91d5d23","26e833371c8e477a9e47164126fd1188","6614eee6d2bc4d9ebb3a39e70459646e","91f1889c911e47acbe2f8572c0e35f7b","410a6049403449e0a4b1250ea6574e44","c1e86fc2e39249db815537544269a937","a564e3c1910d42e3bd684c66f3cc32ba","59c94ccc796c431ab268f3ef85703cff","45e3a13f81974cf296c5f948dd9a4e95","63bf53367320453c89bbf997a73d746a","4f786246fada4ae8b2dcc89913499ff8","ac8d0b2a93f342ce9e667fdae283f244","6a72aa96082a4f16aa7c36fe0a943df6","163e6060218941d98787102261af36c2","822fafba68b449a5bfa043643880e818","75aa5b5c4d9d4480b2454f027d697152","49d88c66b5324e9eb330678e8d54f707","cf48e40c9db7419586181a3c53f88617","4cb7d5505ed643edb80d4df216e060d3","04f3bd47cd814424a5f56bb3ed5098a7","943456a931484e0d978e49be6f797624","2b44eb69c3804c0aa4839902be9c716f"]},"id":"2oBTiJka95B4","executionInfo":{"status":"ok","timestamp":1678033295064,"user_tz":360,"elapsed":17651,"user":{"displayName":"Aren Wilson-Wright","userId":"05722690186564388330"}},"outputId":"7334e367-1a24-48e5-d2f6-a8cc548ad92e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5eaad8b77e4b59b396c0cd847551c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/110k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001d51c7bacf4fdbb73efffd64cf8802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/121 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2ff1fa9700a443bae6b72cfee3caaa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading metadata:   0%|          | 0.00/916 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9029fd7619634a6cb8836132e48954cc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset csv/default (download: 1.08 MiB, generated: 2.56 MiB, post-processed: Unknown size, total: 3.64 MiB) to /root/.cache/huggingface/datasets/gngpostalsrvc___parquet/gngpostalsrvc--COHeN-97096b619f4d4787/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6cc03159e34364b6f5e5e92f5b924c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/120k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b771a6c2146143c7a90a8f9a9e675960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/121k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d4230c14754054ab74971daa0497d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/890k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a703c2b465ba40afb92959ac3ecaf194"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ef90980a9cf404eaa3f27798fa29d60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/1197 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb91d6b584e48878d40aac73b2eddb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating eval split:   0%|          | 0/1197 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21edef14c2cc41c580eec1f3d5361277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/9574 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ccd5ed390744bba6a55be39ac27deb"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/gngpostalsrvc___parquet/gngpostalsrvc--COHeN-97096b619f4d4787/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6980469b22854ee8956e288cb5f49caa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1197 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553147f17d1f484881ae2687349045f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/1197 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffe094769cc941818ece5814b91d5d23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/9574 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac8d0b2a93f342ce9e667fdae283f244"}},"metadata":{}}]},{"cell_type":"code","source":["def compute_metrics(eval_preds):\n","    metrics = evaluate.load('accuracy')\n","    logits, labels = eval_preds\n","    predictions = np.argmax(logits, axis=-1)\n","    return metrics.compute(predictions=predictions, references=labels)\n","\n","\n","def objective(trial):\n","  model = AutoModelForSequenceClassification.from_pretrained('gngpostalsrvc/BERiT', num_labels=4)\n","  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n","  args = TrainingArguments(output_dir=\"opt-test\", \n","                         evaluation_strategy=\"epoch\",\n","                         learning_rate=trial.suggest_float('learning_rate', low=4e-5, high=.01),\n","                         weight_decay=trial.suggest_float('weight_decay', low=4e-5, high=.01),\n","                         num_train_epochs=3,\n","                         per_device_train_batch_size=batch_size, \n","                         per_device_eval_batch_size=batch_size, \n","                         seed=42,\n","                         disable_tqdm=True\n","                        )\n","  \n","  trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized_data['train'],\n","    eval_dataset=tokenized_data['test'],\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n","  )\n","\n","  result = trainer.train()\n","\n","  return result.training_loss\n","\n","study = optuna.create_study(study_name='hp-search-COHeN', direction='minimize')\n","study.optimize(func=objective, n_trials=20)"],"metadata":{"id":"jhs6iC_xSYfb","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d03b87b5e0f9496b90a813e1940639dd","6574990ea8c24f1285dcda77095ebb7f","d31909a4e8ee4ffd8fcba374138b533b","c5fc3829dfff450c9c2edf76330be8d0","c69c862256604dc981304d4b89d91d7d","3950c92254094a38a9fc865f1b118d19","042bc4e51e954fdc8d715179c1939d52","2567dfbd802c4f2db8d643b562591a59","eab7956a6f7444a79fa266b073772d44","060ec577a1a446fda25244cdb8a81bcc","fbeb953e994e4aaa9fb57662c115856c","165eea738200474aaf76e3b7378b21a6","cf052865ef56467fbcdcfef264b430d8","bd73076bbbf446bfb62c21ffa2efcb32","c57635badef046c594539f1f7b864f20","56d22c04089f474ba4efac0e9348c83a","a788a7ba655d47dba1a5156d0afb8ec0","002f0d340c0546acaae8c91a6cdfee2f","b2671b7984854ca0ae2656ce62ee59a1","e8a7bf250f2e4dd78207d5bede4fce91","10e2252355944b4b85215680a10f4030","1decf84b8f294c419f7f318a127a17d8"]},"executionInfo":{"status":"ok","timestamp":1678035853036,"user_tz":360,"elapsed":873370,"user":{"displayName":"Aren Wilson-Wright","userId":"05722690186564388330"}},"outputId":"1f142e1d-c15a-4c8f-82cd-ef28d5095a48"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2023-03-05 16:49:39,626]\u001b[0m A new study created in memory with name: hp-search-COHeN\u001b[0m\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/669 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03b87b5e0f9496b90a813e1940639dd"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/5.68M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"165eea738200474aaf76e3b7378b21a6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1797\n","  Number of trainable parameters = 1414916\n","Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3993, 'learning_rate': 0.005234378203240644, 'epoch': 0.83}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.27681303024292, 'eval_accuracy': 0.4093567251461988, 'eval_runtime': 2.5228, 'eval_samples_per_second': 474.479, 'eval_steps_per_second': 29.729, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1000\n","Configuration saved in opt-test/checkpoint-1000/config.json\n","Model weights saved in opt-test/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3452, 'learning_rate': 0.0032164991734639887, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.2064297199249268, 'eval_accuracy': 0.4578111946532999, 'eval_runtime': 1.85, 'eval_samples_per_second': 647.011, 'eval_steps_per_second': 40.54, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1500\n","Configuration saved in opt-test/checkpoint-1500/config.json\n","Model weights saved in opt-test/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3034, 'learning_rate': 0.0011986201436873334, 'epoch': 2.5}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:50:32,171]\u001b[0m Trial 0 finished with value: 1.3324641003234559 and parameters: {'batch_size': 16, 'learning_rate': 0.0072522572330173, 'weight_decay': 0.007029157880518676}. Best is trial 0 with value: 1.3324641003234559.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.1374976634979248, 'eval_accuracy': 0.49373433583959897, 'eval_runtime': 1.8652, 'eval_samples_per_second': 641.755, 'eval_steps_per_second': 40.21, 'epoch': 3.0}\n","{'train_runtime': 49.7997, 'train_samples_per_second': 576.75, 'train_steps_per_second': 36.085, 'train_loss': 1.3324641003234559, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 450\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.0129808187484741, 'eval_accuracy': 0.568922305764411, 'eval_runtime': 1.7635, 'eval_samples_per_second': 678.751, 'eval_steps_per_second': 10.774, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8812138438224792, 'eval_accuracy': 0.6382623224728488, 'eval_runtime': 1.7371, 'eval_samples_per_second': 689.062, 'eval_steps_per_second': 10.937, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:51:11,596]\u001b[0m Trial 1 finished with value: 1.0931613498263888 and parameters: {'batch_size': 64, 'learning_rate': 0.001330810111532763, 'weight_decay': 0.00572849123009566}. Best is trial 1 with value: 1.0931613498263888.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8670345544815063, 'eval_accuracy': 0.6449456975772765, 'eval_runtime': 1.7281, 'eval_samples_per_second': 692.67, 'eval_steps_per_second': 10.995, 'epoch': 3.0}\n","{'train_runtime': 38.9973, 'train_samples_per_second': 736.513, 'train_steps_per_second': 11.539, 'train_loss': 1.0931613498263888, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3591\n","  Number of trainable parameters = 1414916\n","Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.5465, 'learning_rate': 0.008184938995544847, 'epoch': 0.42}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1000\n","Configuration saved in opt-test/checkpoint-1000/config.json\n","Model weights saved in opt-test/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.4951, 'learning_rate': 0.006860943687304011, 'epoch': 0.84}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.3343666791915894, 'eval_accuracy': 0.3817878028404344, 'eval_runtime': 2.6951, 'eval_samples_per_second': 444.137, 'eval_steps_per_second': 55.656, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1500\n","Configuration saved in opt-test/checkpoint-1500/config.json\n","Model weights saved in opt-test/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.4294, 'learning_rate': 0.005536948379063176, 'epoch': 1.25}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-2000\n","Configuration saved in opt-test/checkpoint-2000/config.json\n","Model weights saved in opt-test/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-2000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.4063, 'learning_rate': 0.0042129530708223395, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.2973289489746094, 'eval_accuracy': 0.3533834586466165, 'eval_runtime': 2.0553, 'eval_samples_per_second': 582.406, 'eval_steps_per_second': 72.983, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-2500\n","Configuration saved in opt-test/checkpoint-2500/config.json\n","Model weights saved in opt-test/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-2500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3698, 'learning_rate': 0.0028889577625815033, 'epoch': 2.09}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-3000\n","Configuration saved in opt-test/checkpoint-3000/config.json\n","Model weights saved in opt-test/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-3000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3258, 'learning_rate': 0.0015649624543406678, 'epoch': 2.51}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-3500\n","Configuration saved in opt-test/checkpoint-3500/config.json\n","Model weights saved in opt-test/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-3500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3075, 'learning_rate': 0.0002409671460998321, 'epoch': 2.92}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:52:18,198]\u001b[0m Trial 2 finished with value: 1.4082931437820354 and parameters: {'batch_size': 8, 'learning_rate': 0.009508934303785683, 'weight_decay': 0.0032297273160342265}. Best is trial 1 with value: 1.0931613498263888.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.228412389755249, 'eval_accuracy': 0.454469507101086, 'eval_runtime': 2.102, 'eval_samples_per_second': 569.466, 'eval_steps_per_second': 71.362, 'epoch': 3.0}\n","{'train_runtime': 66.1683, 'train_samples_per_second': 434.075, 'train_steps_per_second': 54.271, 'train_loss': 1.4082931437820354, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 225\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 128\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9793399572372437, 'eval_accuracy': 0.5973266499582289, 'eval_runtime': 2.3168, 'eval_samples_per_second': 516.664, 'eval_steps_per_second': 4.316, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 128\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.804386556148529, 'eval_accuracy': 0.6892230576441103, 'eval_runtime': 2.1088, 'eval_samples_per_second': 567.626, 'eval_steps_per_second': 4.742, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 128\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:52:52,433]\u001b[0m Trial 3 finished with value: 1.0458947075737848 and parameters: {'batch_size': 128, 'learning_rate': 0.0037823661822296854, 'weight_decay': 0.008199998128890522}. Best is trial 3 with value: 1.0458947075737848.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7615264654159546, 'eval_accuracy': 0.7134502923976608, 'eval_runtime': 1.7089, 'eval_samples_per_second': 700.469, 'eval_steps_per_second': 5.852, 'epoch': 3.0}\n","{'train_runtime': 33.7945, 'train_samples_per_second': 849.902, 'train_steps_per_second': 6.658, 'train_loss': 1.0458947075737848, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1797\n","  Number of trainable parameters = 1414916\n","Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3092, 'learning_rate': 0.002716816541516378, 'epoch': 0.83}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.1558396816253662, 'eval_accuracy': 0.48872180451127817, 'eval_runtime': 2.657, 'eval_samples_per_second': 450.51, 'eval_steps_per_second': 28.227, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1000\n","Configuration saved in opt-test/checkpoint-1000/config.json\n","Model weights saved in opt-test/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.2182, 'learning_rate': 0.001669470149258715, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.019949197769165, 'eval_accuracy': 0.5405179615705932, 'eval_runtime': 1.8509, 'eval_samples_per_second': 646.712, 'eval_steps_per_second': 40.521, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1500\n","Configuration saved in opt-test/checkpoint-1500/config.json\n","Model weights saved in opt-test/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.1226, 'learning_rate': 0.0006221237570010519, 'epoch': 2.5}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:53:42,837]\u001b[0m Trial 4 finished with value: 1.190702120993757 and parameters: {'batch_size': 16, 'learning_rate': 0.0037641629337740413, 'weight_decay': 0.0034939299191096953}. Best is trial 3 with value: 1.0458947075737848.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9604802131652832, 'eval_accuracy': 0.5772765246449457, 'eval_runtime': 1.8554, 'eval_samples_per_second': 645.144, 'eval_steps_per_second': 40.423, 'epoch': 3.0}\n","{'train_runtime': 49.9684, 'train_samples_per_second': 574.803, 'train_steps_per_second': 35.963, 'train_loss': 1.190702120993757, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 128\n","  Total train batch size (w. parallel, distributed & accumulation) = 128\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 225\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 128\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.0737048387527466, 'eval_accuracy': 0.5071010860484545, 'eval_runtime': 1.7075, 'eval_samples_per_second': 701.027, 'eval_steps_per_second': 5.857, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 128\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8680345416069031, 'eval_accuracy': 0.6357560568086884, 'eval_runtime': 1.7446, 'eval_samples_per_second': 686.118, 'eval_steps_per_second': 5.732, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 128\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:54:19,295]\u001b[0m Trial 5 finished with value: 1.1131176079644096 and parameters: {'batch_size': 128, 'learning_rate': 0.008490164173397822, 'weight_decay': 0.0003297536762993297}. Best is trial 3 with value: 1.0458947075737848.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8010269999504089, 'eval_accuracy': 0.6825396825396826, 'eval_runtime': 2.3198, 'eval_samples_per_second': 515.993, 'eval_steps_per_second': 4.311, 'epoch': 3.0}\n","{'train_runtime': 36.0157, 'train_samples_per_second': 797.485, 'train_steps_per_second': 6.247, 'train_loss': 1.1131176079644096, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9577077627182007, 'eval_accuracy': 0.6106934001670844, 'eval_runtime': 2.0976, 'eval_samples_per_second': 570.66, 'eval_steps_per_second': 18.116, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.115, 'learning_rate': 0.0008670800786538124, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.808030903339386, 'eval_accuracy': 0.6775271512113618, 'eval_runtime': 1.9866, 'eval_samples_per_second': 602.528, 'eval_steps_per_second': 19.128, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:55:00,857]\u001b[0m Trial 6 finished with value: 1.0253081936306423 and parameters: {'batch_size': 32, 'learning_rate': 0.001950930176971078, 'weight_decay': 0.007649742251688652}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7304974794387817, 'eval_accuracy': 0.7184628237259816, 'eval_runtime': 1.9614, 'eval_samples_per_second': 610.292, 'eval_steps_per_second': 19.374, 'epoch': 3.0}\n","{'train_runtime': 41.1243, 'train_samples_per_second': 698.418, 'train_steps_per_second': 21.885, 'train_loss': 1.0253081936306423, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 450\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9491210579872131, 'eval_accuracy': 0.6123642439431913, 'eval_runtime': 2.19, 'eval_samples_per_second': 546.577, 'eval_steps_per_second': 8.676, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7972204089164734, 'eval_accuracy': 0.7034252297410192, 'eval_runtime': 2.4542, 'eval_samples_per_second': 487.741, 'eval_steps_per_second': 7.742, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:55:37,957]\u001b[0m Trial 7 finished with value: 1.0445826551649307 and parameters: {'batch_size': 64, 'learning_rate': 0.0043985276086475926, 'weight_decay': 0.00027089057483651045}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7274888753890991, 'eval_accuracy': 0.7376775271512114, 'eval_runtime': 2.0447, 'eval_samples_per_second': 585.425, 'eval_steps_per_second': 9.292, 'epoch': 3.0}\n","{'train_runtime': 36.6608, 'train_samples_per_second': 783.454, 'train_steps_per_second': 12.275, 'train_loss': 1.0445826551649307, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3591\n","  Number of trainable parameters = 1414916\n","Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.5618, 'learning_rate': 0.008425511214532925, 'epoch': 0.42}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1000\n","Configuration saved in opt-test/checkpoint-1000/config.json\n","Model weights saved in opt-test/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.5068, 'learning_rate': 0.007062600956601361, 'epoch': 0.84}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.4408103227615356, 'eval_accuracy': 0.2807017543859649, 'eval_runtime': 2.0457, 'eval_samples_per_second': 585.121, 'eval_steps_per_second': 73.323, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-1500\n","Configuration saved in opt-test/checkpoint-1500/config.json\n","Model weights saved in opt-test/checkpoint-1500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-1500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-1500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.4681, 'learning_rate': 0.0056996906986697985, 'epoch': 1.25}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-2000\n","Configuration saved in opt-test/checkpoint-2000/config.json\n","Model weights saved in opt-test/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-2000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.4284, 'learning_rate': 0.004336780440738234, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 8\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.2848174571990967, 'eval_accuracy': 0.36006683375104426, 'eval_runtime': 2.1205, 'eval_samples_per_second': 564.499, 'eval_steps_per_second': 70.739, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-2500\n","Configuration saved in opt-test/checkpoint-2500/config.json\n","Model weights saved in opt-test/checkpoint-2500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-2500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-2500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3835, 'learning_rate': 0.0029738701828066713, 'epoch': 2.09}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-3000\n","Configuration saved in opt-test/checkpoint-3000/config.json\n","Model weights saved in opt-test/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-3000/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3505, 'learning_rate': 0.0016109599248751078, 'epoch': 2.51}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-3500\n","Configuration saved in opt-test/checkpoint-3500/config.json\n","Model weights saved in opt-test/checkpoint-3500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-3500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-3500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.324, 'learning_rate': 0.0002480496669435445, 'epoch': 2.92}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 8\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:56:43,830]\u001b[0m Trial 8 finished with value: 1.4286482132965062 and parameters: {'batch_size': 8, 'learning_rate': 0.009788421472464488, 'weight_decay': 0.0014730071705606927}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.248661994934082, 'eval_accuracy': 0.4319131161236424, 'eval_runtime': 2.07, 'eval_samples_per_second': 578.267, 'eval_steps_per_second': 72.464, 'epoch': 3.0}\n","{'train_runtime': 65.3764, 'train_samples_per_second': 439.333, 'train_steps_per_second': 54.928, 'train_loss': 1.4286482132965062, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.1214994192123413, 'eval_accuracy': 0.5405179615705932, 'eval_runtime': 1.7709, 'eval_samples_per_second': 675.937, 'eval_steps_per_second': 21.458, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.2125, 'learning_rate': 0.0021481494886553152, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8886598348617554, 'eval_accuracy': 0.6491228070175439, 'eval_runtime': 1.7997, 'eval_samples_per_second': 665.123, 'eval_steps_per_second': 21.115, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:57:25,281]\u001b[0m Trial 9 finished with value: 1.10341305202908 and parameters: {'batch_size': 32, 'learning_rate': 0.0048333363494744595, 'weight_decay': 0.005670945427929289}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7928232550621033, 'eval_accuracy': 0.6850459482038429, 'eval_runtime': 1.8164, 'eval_samples_per_second': 658.996, 'eval_steps_per_second': 20.921, 'epoch': 3.0}\n","{'train_runtime': 41.0087, 'train_samples_per_second': 700.388, 'train_steps_per_second': 21.947, 'train_loss': 1.10341305202908, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.3222905397415161, 'eval_accuracy': 0.4452798663324979, 'eval_runtime': 1.8043, 'eval_samples_per_second': 663.425, 'eval_steps_per_second': 21.061, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.3476, 'learning_rate': 3.084038117453277e-05, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.239760398864746, 'eval_accuracy': 0.4394319131161236, 'eval_runtime': 1.8188, 'eval_samples_per_second': 658.128, 'eval_steps_per_second': 20.893, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:58:05,917]\u001b[0m Trial 10 finished with value: 1.3105258178710937 and parameters: {'batch_size': 32, 'learning_rate': 6.939085764269873e-05, 'weight_decay': 0.009495696149381526}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.2284375429153442, 'eval_accuracy': 0.4452798663324979, 'eval_runtime': 1.8117, 'eval_samples_per_second': 660.712, 'eval_steps_per_second': 20.975, 'epoch': 3.0}\n","{'train_runtime': 40.1522, 'train_samples_per_second': 715.329, 'train_steps_per_second': 22.415, 'train_loss': 1.3105258178710937, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 450\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.0283247232437134, 'eval_accuracy': 0.5789473684210527, 'eval_runtime': 1.7453, 'eval_samples_per_second': 685.846, 'eval_steps_per_second': 10.886, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.835058331489563, 'eval_accuracy': 0.6641604010025063, 'eval_runtime': 1.7316, 'eval_samples_per_second': 691.26, 'eval_steps_per_second': 10.972, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:58:43,721]\u001b[0m Trial 11 finished with value: 1.0532293701171875 and parameters: {'batch_size': 64, 'learning_rate': 0.0023967333518982313, 'weight_decay': 0.0036558052101246423}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7766752243041992, 'eval_accuracy': 0.6992481203007519, 'eval_runtime': 1.7371, 'eval_samples_per_second': 689.067, 'eval_steps_per_second': 10.938, 'epoch': 3.0}\n","{'train_runtime': 37.3236, 'train_samples_per_second': 769.539, 'train_steps_per_second': 12.057, 'train_loss': 1.0532293701171875, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.1547768115997314, 'eval_accuracy': 0.49707602339181284, 'eval_runtime': 1.7768, 'eval_samples_per_second': 673.682, 'eval_steps_per_second': 21.387, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.2767, 'learning_rate': 0.0026859036303730674, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.1116973161697388, 'eval_accuracy': 0.5121136173767753, 'eval_runtime': 1.878, 'eval_samples_per_second': 637.366, 'eval_steps_per_second': 20.234, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 16:59:25,272]\u001b[0m Trial 12 finished with value: 1.2152385457356771 and parameters: {'batch_size': 32, 'learning_rate': 0.006043283168339402, 'weight_decay': 0.007290632407032851}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.0361379384994507, 'eval_accuracy': 0.5664160401002506, 'eval_runtime': 1.7767, 'eval_samples_per_second': 673.74, 'eval_steps_per_second': 21.389, 'epoch': 3.0}\n","{'train_runtime': 41.0891, 'train_samples_per_second': 699.018, 'train_steps_per_second': 21.904, 'train_loss': 1.2152385457356771, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 450\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.0106077194213867, 'eval_accuracy': 0.5831244778613199, 'eval_runtime': 1.739, 'eval_samples_per_second': 688.319, 'eval_steps_per_second': 10.926, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8403245806694031, 'eval_accuracy': 0.6700083542188805, 'eval_runtime': 1.7403, 'eval_samples_per_second': 687.812, 'eval_steps_per_second': 10.918, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:00:04,012]\u001b[0m Trial 13 finished with value: 1.0479649522569445 and parameters: {'batch_size': 64, 'learning_rate': 0.0025119036826251875, 'weight_decay': 0.00011464331705632672}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7568548917770386, 'eval_accuracy': 0.7176274018379282, 'eval_runtime': 1.7261, 'eval_samples_per_second': 693.474, 'eval_steps_per_second': 11.008, 'epoch': 3.0}\n","{'train_runtime': 38.2881, 'train_samples_per_second': 750.155, 'train_steps_per_second': 11.753, 'train_loss': 1.0479649522569445, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.084716558456421, 'eval_accuracy': 0.5271512113617377, 'eval_runtime': 1.8003, 'eval_samples_per_second': 664.875, 'eval_steps_per_second': 21.107, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.2434, 'learning_rate': 0.0024120922310264734, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9552063941955566, 'eval_accuracy': 0.5914786967418546, 'eval_runtime': 1.7764, 'eval_samples_per_second': 673.842, 'eval_steps_per_second': 21.392, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:00:45,878]\u001b[0m Trial 14 finished with value: 1.1480138821072048 and parameters: {'batch_size': 32, 'learning_rate': 0.005427207519809565, 'weight_decay': 0.009860776929397401}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8361523747444153, 'eval_accuracy': 0.6516290726817042, 'eval_runtime': 1.8202, 'eval_samples_per_second': 657.615, 'eval_steps_per_second': 20.877, 'epoch': 3.0}\n","{'train_runtime': 41.4115, 'train_samples_per_second': 693.576, 'train_steps_per_second': 21.733, 'train_loss': 1.1480138821072048, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 450\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.022154688835144, 'eval_accuracy': 0.5864661654135338, 'eval_runtime': 1.7165, 'eval_samples_per_second': 697.338, 'eval_steps_per_second': 11.069, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7917675375938416, 'eval_accuracy': 0.7000835421888053, 'eval_runtime': 2.117, 'eval_samples_per_second': 565.424, 'eval_steps_per_second': 8.975, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:01:26,660]\u001b[0m Trial 15 finished with value: 1.0415825737847222 and parameters: {'batch_size': 64, 'learning_rate': 0.0038487109526380526, 'weight_decay': 0.004897398355058994}. Best is trial 6 with value: 1.0253081936306423.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7296342253684998, 'eval_accuracy': 0.7326649958228906, 'eval_runtime': 1.7554, 'eval_samples_per_second': 681.905, 'eval_steps_per_second': 10.824, 'epoch': 3.0}\n","{'train_runtime': 40.3278, 'train_samples_per_second': 712.214, 'train_steps_per_second': 11.159, 'train_loss': 1.0415825737847222, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.930658757686615, 'eval_accuracy': 0.6282372598162071, 'eval_runtime': 1.7913, 'eval_samples_per_second': 668.222, 'eval_steps_per_second': 21.213, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.111, 'learning_rate': 0.00121917744310983, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7783859968185425, 'eval_accuracy': 0.7000835421888053, 'eval_runtime': 1.7852, 'eval_samples_per_second': 670.508, 'eval_steps_per_second': 21.286, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:02:08,576]\u001b[0m Trial 16 finished with value: 1.0100221082899306 and parameters: {'batch_size': 32, 'learning_rate': 0.0027431492469971175, 'weight_decay': 0.004900150335195089}. Best is trial 16 with value: 1.0100221082899306.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7056787014007568, 'eval_accuracy': 0.7301587301587301, 'eval_runtime': 1.7986, 'eval_samples_per_second': 665.514, 'eval_steps_per_second': 21.127, 'epoch': 3.0}\n","{'train_runtime': 41.4601, 'train_samples_per_second': 692.763, 'train_steps_per_second': 21.708, 'train_loss': 1.0100221082899306, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9249110817909241, 'eval_accuracy': 0.6165413533834586, 'eval_runtime': 1.7944, 'eval_samples_per_second': 667.086, 'eval_steps_per_second': 21.177, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.1138, 'learning_rate': 0.0010398614716122443, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7815350294113159, 'eval_accuracy': 0.6716791979949874, 'eval_runtime': 1.774, 'eval_samples_per_second': 674.737, 'eval_steps_per_second': 21.42, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:02:50,560]\u001b[0m Trial 17 finished with value: 1.0171932305230034 and parameters: {'batch_size': 32, 'learning_rate': 0.00233968831112755, 'weight_decay': 0.0068614372121561535}. Best is trial 16 with value: 1.0100221082899306.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7292386889457703, 'eval_accuracy': 0.7151211361737677, 'eval_runtime': 1.8138, 'eval_samples_per_second': 659.954, 'eval_steps_per_second': 20.951, 'epoch': 3.0}\n","{'train_runtime': 41.5276, 'train_samples_per_second': 691.637, 'train_steps_per_second': 21.672, 'train_loss': 1.0171932305230034, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 1.0313823223114014, 'eval_accuracy': 0.5647451963241437, 'eval_runtime': 1.8176, 'eval_samples_per_second': 658.565, 'eval_steps_per_second': 20.907, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.1652, 'learning_rate': 0.0002635496500834576, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9588459730148315, 'eval_accuracy': 0.6215538847117794, 'eval_runtime': 1.8634, 'eval_samples_per_second': 642.359, 'eval_steps_per_second': 20.392, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:03:31,447]\u001b[0m Trial 18 finished with value: 1.1151045735677083 and parameters: {'batch_size': 32, 'learning_rate': 0.0005929867126877797, 'weight_decay': 0.006269757810787477}. Best is trial 16 with value: 1.0100221082899306.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9213922023773193, 'eval_accuracy': 0.6357560568086884, 'eval_runtime': 1.8014, 'eval_samples_per_second': 664.474, 'eval_steps_per_second': 21.094, 'epoch': 3.0}\n","{'train_runtime': 40.3856, 'train_samples_per_second': 711.194, 'train_steps_per_second': 22.285, 'train_loss': 1.1151045735677083, 'epoch': 3.0}\n"]},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/config.json\n","Model config RobertaConfig {\n","  \"_name_or_path\": \"gngpostalsrvc/BERiT\",\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.5,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.5,\n","  \"hidden_size\": 256,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\",\n","    \"3\": \"LABEL_3\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 1024,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2,\n","    \"LABEL_3\": 3\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 128,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 4,\n","  \"num_hidden_layers\": 1,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.1\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 2050\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--gngpostalsrvc--BERiT/snapshots/5799f2933c5845e55984f628ee81b985752c3897/pytorch_model.bin\n","Some weights of the model checkpoint at gngpostalsrvc/BERiT were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at gngpostalsrvc/BERiT and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 9574\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 900\n","  Number of trainable parameters = 1414916\n","***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.9311962723731995, 'eval_accuracy': 0.6157059314954052, 'eval_runtime': 2.2046, 'eval_samples_per_second': 542.948, 'eval_steps_per_second': 17.236, 'epoch': 1.0}\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to opt-test/checkpoint-500\n","Configuration saved in opt-test/checkpoint-500/config.json\n","Model weights saved in opt-test/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in opt-test/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in opt-test/checkpoint-500/special_tokens_map.json\n"]},{"output_type":"stream","name":"stdout","text":["{'loss': 1.1347, 'learning_rate': 0.001438975144983106, 'epoch': 1.67}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.8189311027526855, 'eval_accuracy': 0.6666666666666666, 'eval_runtime': 2.4248, 'eval_samples_per_second': 493.649, 'eval_steps_per_second': 15.671, 'epoch': 2.0}\n"]},{"output_type":"stream","name":"stderr","text":["***** Running Evaluation *****\n","  Num examples = 1197\n","  Batch size = 32\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\u001b[32m[I 2023-03-05 17:04:12,729]\u001b[0m Trial 19 finished with value: 1.0276928032769097 and parameters: {'batch_size': 32, 'learning_rate': 0.0032376940762119883, 'weight_decay': 0.005012474188757684}. Best is trial 16 with value: 1.0100221082899306.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["{'eval_loss': 0.7353609204292297, 'eval_accuracy': 0.720969089390142, 'eval_runtime': 2.4489, 'eval_samples_per_second': 488.799, 'eval_steps_per_second': 15.517, 'epoch': 3.0}\n","{'train_runtime': 40.7823, 'train_samples_per_second': 704.277, 'train_steps_per_second': 22.068, 'train_loss': 1.0276928032769097, 'epoch': 3.0}\n"]}]},{"cell_type":"markdown","source":["{'batch_size': 32, 'learning_rate': 0.0027431492469971175, 'weight_decay': 0.004900150335195089}"],"metadata":{"id":"ucvLsShHD0XA"}}]}